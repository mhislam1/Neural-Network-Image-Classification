{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE474/574 - Programming Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Sentiment Analysis\n",
    "\n",
    "In the code provided below, you need to add code wherever specified by `TODO:`. \n",
    "\n",
    "> You will be using a Python collection class - `Counter` to maintain the word counts. \n",
    "\n",
    "> See https://docs.python.org/2/library/collections.html for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data files \n",
    "g = open('reviews.txt','r') # What we know!\n",
    "reviews_all = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "g = open('labels.txt','r') # What we WANT to know!\n",
    "sentiments_all = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()\n",
    "\n",
    "# load vocabulary\n",
    "g = open('vocab.txt','r')\n",
    "vocab = [s.strip() for s in g.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a set of 25000 movie reviews, along with a `POSITIVE` or `NEGATIVE` sentiment label assigned to the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A POSITIVE review:\n",
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "\n",
      "A NEGATIVE review:\n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n"
     ]
    }
   ],
   "source": [
    "# Check out sample reviews\n",
    "print('A {} review:'.format(sentiments_all[0]))\n",
    "print(reviews_all[0])\n",
    "print('\\nA {} review:'.format(sentiments_all[1]))\n",
    "print(reviews_all[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\n"
     ]
    }
   ],
   "source": [
    "# split into training and test data\n",
    "reviews_train,reviews_test = reviews_all[0:24000],reviews_all[24000:]\n",
    "sentiments_train,sentiments_test = sentiments_all[0:24000],sentiments_all[24000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintain Counter objects to store positive, negative and total counts for\n",
    "# all the words present in the positive, negative and total reviews.\n",
    "positive_word_count = Counter()\n",
    "negative_word_count = Counter()\n",
    "total_counts = Counter()\n",
    "# TODO: Loop over all the words in the vocabulary\n",
    "# and increment the counts in the appropriate counter objects\n",
    "# based on the training data\n",
    "\n",
    "\n",
    "i = 0\n",
    "word = ''\n",
    "while i < len(sentiments_train):   \n",
    "    if sentiments_train[i] == 'POSITIVE':\n",
    "        for letter in reviews_train[i]:            \n",
    "            if letter == ' ':\n",
    "                if not word.isalpha():\n",
    "                    word = ''\n",
    "                    continue\n",
    "                    \n",
    "                positive_word_count[word] = positive_word_count[word] + 1\n",
    "                total_counts[word] =  total_counts[word] + 1\n",
    "                word = ''\n",
    "            else:\n",
    "                word = word + letter\n",
    "            \n",
    "    else:\n",
    "        for letter in reviews_train[i]:\n",
    "            if letter == ' ':\n",
    "                if not word.isalpha():\n",
    "                    word = ''\n",
    "                    continue\n",
    "                negative_word_count[word] = negative_word_count[word] + 1\n",
    "                total_counts[word] =  total_counts[word] + 1\n",
    "                word = ''\n",
    "            else:\n",
    "                word = word + letter\n",
    "            \n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintain a Counter object to store positive to negative ratios \n",
    "pos_neg_ratios = Counter()\n",
    "\n",
    "# Calculate the ratios of positive and negative uses of the most common words\n",
    "# Consider words to be \"common\" if they've been used at least 100 times\n",
    "for term,cnt in list(total_counts.most_common()):\n",
    "    if(cnt > 100):\n",
    "        # TODO: Code for calculating the ratios (remove the next line)\n",
    "        posVal = positive_word_count[term]\n",
    "        if posVal == 0:\n",
    "            posVal = 1\n",
    "            \n",
    "        negVal = negative_word_count[term]\n",
    "        if negVal == 0:\n",
    "            negVal = 1\n",
    "            \n",
    "        ratio = posVal / negVal\n",
    "        pos_neg_ratios[term] = ratio\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos-to-neg ratio for 'the' = 1.0618582280413789\n",
      "Pos-to-neg ratio for 'amazing' = 4.031496062992126\n",
      "Pos-to-neg ratio for 'terrible' = 0.17256637168141592\n"
     ]
    }
   ],
   "source": [
    "print(\"Pos-to-neg ratio for 'the' = {}\".format(pos_neg_ratios[\"the\"]))\n",
    "print(\"Pos-to-neg ratio for 'amazing' = {}\".format(pos_neg_ratios[\"amazing\"]))\n",
    "print(\"Pos-to-neg ratio for 'terrible' = {}\".format(pos_neg_ratios[\"terrible\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a log of the ratio\n",
    "for word,ratio in pos_neg_ratios.most_common():\n",
    "    pos_neg_ratios[word] = np.log(ratio)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the distribution of the log-ratio scores\n",
    "scores = np.array(list(pos_neg_ratios.values()))\n",
    "vocab_selected = list(pos_neg_ratios.keys())\n",
    "\n",
    "h = plt.hist(scores,bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above histogram should give you an idea about the distribution of the scores.\n",
    "\n",
    "Notice how the scores are distributed around 0. A word with score 0 can be considered as `neutral`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose\n",
      "hands\n",
      "beat\n",
      "onto\n",
      "test\n",
      "equal\n",
      "basis\n",
      "psycho\n",
      "speech\n",
      "kane\n"
     ]
    }
   ],
   "source": [
    "# Print few words with neutral score\n",
    "for ind in np.where(scores == 0)[0][0:10]:\n",
    "    print(vocab_selected[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**APPROACH 1** Implement a simple non-machine learning that only uses the log-ratios to determine if a review is positive or negative. This function will be applied to the test data to calculate the accuracy of the model. \n",
    "\n",
    "_See the assignment document for hints._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonml_classifier(review,pos_neg_ratios):\n",
    "    '''\n",
    "    Function that determines the sentiment for a given review.\n",
    "    \n",
    "    Inputs:\n",
    "      review - A text containing a movie review\n",
    "      pos_neg_ratios - A Counter object containing frequent words\n",
    "                       and corresponding log positive-negative ratio\n",
    "    Return:\n",
    "      sentiment - 'NEGATIVE' or 'POSITIVE'\n",
    "    '''\n",
    "    # TODO: Implement the algorithm here. Change the next line.\n",
    "    decider = 0\n",
    "    word = ''\n",
    "    for letter in review:            \n",
    "            if letter == ' ':\n",
    "                if not word.isalpha():\n",
    "                    word = ''\n",
    "                    continue\n",
    "                decider = decider + pos_neg_ratios[word]\n",
    "                word = ''\n",
    "            else:\n",
    "                word = word + letter\n",
    "    if decider < 0:\n",
    "        return 'NEGATIVE'\n",
    "    else:\n",
    "        return 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model = 0.763\n"
     ]
    }
   ],
   "source": [
    "predictions_test = []\n",
    "for r in reviews_test:\n",
    "    l = nonml_classifier(r,pos_neg_ratios)\n",
    "    predictions_test.append(l)\n",
    "\n",
    "# calculate accuracy\n",
    "correct = 0\n",
    "for l,p in zip(sentiments_test,predictions_test):\n",
    "    if l == p:\n",
    "        correct = correct + 1\n",
    "print('Accuracy of the model = {}'.format(correct/len(sentiments_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2** Implement a neural network for sentiment classification. \n",
    "\n",
    "> ### System Configuration\n",
    "This part requires you to use a computer with `tensorflow` library installed. More information is available here - https://www.tensorflow.org.\n",
    "`\n",
    "You are allowed to implement the project on your personal computers using `Python 3.4 or above. You will need `numpy` and `scipy` libraries. If you need to use departmental resources, you can use **metallica.cse.buffalo.edu**, which has `Python 3.4.3` and the required libraries installed. \n",
    "\n",
    "> Students attempting to use the `tensorflow` library have two options: \n",
    "1. Install `tensorflow` on personal machines. Detailed installation information is here - https://www.tensorflow.org/. Note that, since `tensorflow` is a relatively new library, you might encounter installation issues depending on your OS and other library versions. We will not be providing any detailed support regarding `tensorflow` installation. If issues persist, we recommend using option 2. \n",
    "2. Use **metallica.cse.buffalo.edu**. If you are registered into the class, you should have an account on that server. The server already has Python 3.4.3 and TensorFlow 0.12.1 installed. Please use /util/bin/python for Python 3. \n",
    "3. To maintain a ssh connection for a long-running task on a remote machine, use tools like `screen`. For more information: https://linuxize.com/post/how-to-use-linux-screen/ \n",
    "4. For running jupyter-notebook over a remote machine find information on: https://fizzylogic.nl/2017/11/06/edit-jupyter-notebooks-over-ssh/\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_vector(review,word2index):\n",
    "    '''\n",
    "    Function to count how many times each word is used in the given review,\n",
    "    # and then store those counts at the appropriate indices inside x.\n",
    "    '''\n",
    "    vocab_size = len(word2index)\n",
    "    x = np.zeros((1, vocab_size))\n",
    "    for w in review.split(' '):\n",
    "        if w in word2index.keys():\n",
    "            x[0][word2index[w]] += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ignore_words(pos_neg_ratios):\n",
    "    '''\n",
    "    Function to identify words to ignore from the vocabulary\n",
    "    '''\n",
    "    ignore_words = []\n",
    "    # TODO: Complete the implementation of find_ignore_words\n",
    "    for word,ratio in pos_neg_ratios.most_common():\n",
    "        if ratio < 0.1 and ratio > 0:\n",
    "            ignore_words.append(word)\n",
    "    print(ignore_words)\n",
    "    return ignore_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hold', 'screen', 'suited', 'trapped', 'jobs', 'virgin', 'month', 'express', 'occasion', 'aged', 'aspect', 'an', 'offered', 'teenager', 'revealed', 'cross', 'suspects', 'garden', 'place', 'set', 'disturbing', 'briefly', 'population', 'company', 'expert', 'death', 'jesse', 'walls', 'falling', 'incident', 'see', 'anderson', 'specific', 'keep', 'eyes', 'faces', 'dad', 'neighborhood', 'film', 'lot', 'belongs', 'adding', 'nick', 'network', 'fallen', 'wish', 'big', 'wants', 'majority', 'suffered', 'r', 'three', 'logical', 'robot', 'which', 'government', 'performers', 'infamous', 'canadian', 'design', 'stars', 'pop', 'villains', 'extras', 'woman', 'g', 'relative', 'hitler', 'actress', 'provide', 'close', 'whether', 'lisa', 'kate', 'typically', 'wall', 'glass', 'post', 'since', 'combat', 'suspect', 'chris', 'relevant', 'dawn', 'listening', 'cute', 'hiding', 'serve', 's', 'hits', 'mix', 'rate', 'trick', 'wake', 'stole', 'my', 'accepted', 'does', 'horses', 'ups', 'yesterday', 'discussion', 'captain', 'runs', 'decides', 'situations', 'water', 'till', 'second', 'part', 'odd', 'account', 'placed', 'control', 'susan', 'football', 'little', 'consider', 'returning', 'field', 'mission', 'took', 'laura', 'filmed', 'ran', 'under', 'the', 'room', 'scale', 'careers', 'measure', 'liners', 'suffering', 'lost', 'order', 'hat', 'prefer', 'focusing', 'causing', 'suspense', 'super', 'station', 'funeral', 'witch', 'table', 'done', 'taking', 'stephen', 'regret', 'reasons', 'shocking', 'seventies', 'horse', 'heavy', 'create', 'chaplin', 'prior', 'absence', 'matthew', 'jewish', 'white', 'sold', 'walker', 'a', 'time', 'cartoons', 'inspector', 'strangely', 'back', 'wonders', 'speed', 'extent', 'effect', 'cage', 'arts', 'opportunity', 'canada', 'now', 'agent', 'martial', 'camp', 'broken', 'knew', 'featured', 'ultimately', 'contact', 'enjoyment', 'suggests', 'zone', 'references', 'granted', 'separate', 'means', 'type', 'church', 'without', 'right', 'texas', 'catches', 'creepy', 'themselves', 'react', 'numerous', 'fight', 'bang', 'spanish', 'produced', 'regard', 'third', 'entertain', 'amazingly', 'audio', 'folk', 'described', 'one', 'east', 'seen', 'screenplay', 'exaggerated', 'distance', 'wondered', 'sam', 'grandmother', 'added', 'tim', 'old', 'planning', 'further', 'we', 'cameo', 'actor', 'good', 'buddy', 'savage', 'taken', 'state', 'results', 'latest', 'skills', 'house', 'found', 'supernatural', 'leave', 'mind', 'ms', 'comes', 'l', 'hearing', 'fighting', 'revolves', 'interviews', 'face', 'food', 'cases', 'mentally', 'directors', 'guard', 'breaks', 'for', 'such', 'dr', 'whilst', 'however', 'acted', 'buy', 'changing', 'leaving', 'local', 'before', 'co', 'nightmares', 'fool', 'market', 'sticks', 'send', 'daughter', 'dancers', 'clues', 'los', 'jennifer', 'scenery', 'sexual', 'weight', 'cinematic', 'roberts', 'connected', 'dressed', 'soft', 'exists', 'lane', 'slapstick', 'flying', 'compared', 'pain', 'delivery', 'minds', 'followed', 'gas', 'occasional', 'across', 'nearly', 'climax', 'covered', 'tv', 'pity', 'delivered', 'hear', 'constant', 'are', 'into', 'accident', 'jason', 'x', 'forget', 'character', 'clear', 'likes', 'open', 'way', 'general', 'almost']\n"
     ]
    }
   ],
   "source": [
    "# create a word2index mapping from word to an integer index\n",
    "word2index = {}\n",
    "ignore_words = find_ignore_words(pos_neg_ratios)\n",
    "vocab_selected = list(set(vocab_selected).difference(set(ignore_words)))\n",
    "for i,word in enumerate(vocab_selected):\n",
    "    if word not in ignore_words:\n",
    "        word2index[word] = i\n",
    "vocab_size = len(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate .hdf5 files from the processed data\n",
    "Given that the data is moderately large sized, the `hdf5` file format provides a more efficient file representation for further processing. See [here](https://anaconda.org/anaconda/hdf5) for more details and installation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script once to generate the file \n",
    "# delete the exiting 'data1.hdf5' file before running it again to avoid error \n",
    "labels_train = np.zeros((len(sentiments_train), 2), dtype=int)\n",
    "labels_test = np.zeros((len(sentiments_test), 2), dtype=int)\n",
    "\n",
    "with h5py.File('data1.hdf5', 'w') as hf:\n",
    "    hf.create_dataset('data_train', (labels_train.shape[0], vocab_size), np.int16)\n",
    "    hf.create_dataset('data_test', (labels_test.shape[0], vocab_size), np.int16)\n",
    "    # create training data\n",
    "    for i,(r,l) in enumerate(zip(reviews_train, sentiments_train)):\n",
    "        hf[\"data_train\"][i] = create_input_vector(r,word2index)\n",
    "        # one-hot encoding\n",
    "        if l == 'NEGATIVE':\n",
    "            labels_train[i, 0] = 1\n",
    "        else:\n",
    "            labels_train[i, 1] = 1\n",
    "    # create test data\n",
    "    for i,(r,l) in enumerate(zip(reviews_test, sentiments_test)):\n",
    "        hf[\"data_test\"][i] = create_input_vector(r,word2index)\n",
    "        # one-hot encoding\n",
    "        if l == 'NEGATIVE':\n",
    "            labels_test[i, 0] = 1\n",
    "        else:\n",
    "            labels_test[i, 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "tf.compat.v1.random.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the network\n",
    "learning_rate = 0.01\n",
    "batch_size = 400\n",
    "num_epochs = 50 #100 value used to test adding additional epochs\n",
    "n_input = vocab_size\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights and biases in Tensorflow according to the parameters set above\n",
    "n_hidden_1 = 10  # 1st layer number of neurons\n",
    "                #15 value used to add more units to layer\n",
    "#n_hidden_2 = 10\n",
    "#n_hidden_3 = 10\n",
    "weights = {\n",
    "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    #'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),#added layer weights\n",
    "    #'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),#added layer weights\n",
    "\t'out1': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    #'b2': tf.Variable(tf.random_normal([n_hidden_2])),#added layer biases\n",
    "    #'b3': tf.Variable(tf.random_normal([n_hidden_3])),#added layer biases\n",
    "\t'out2': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x):\n",
    "    # define the layers of a single layer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    #layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    #layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    out_layer = tf.nn.sigmoid(tf.matmul(layer_1, weights['out1']) + biases['out2'])\n",
    "    \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = multilayer_perceptron(X)\n",
    "# Define loss(softmax_cross_entropy_with_logits) and optimizer(AdamOptimizer)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some macosx installations, conflicting copies of mpilib causes trouble with tensorflow.\n",
    "# use the following two lines to resolve that issue\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.540833, Test_acc: 0.645000\n",
      "Train acc: 0.726708, Test_acc: 0.756250\n",
      "Train acc: 0.807167, Test_acc: 0.796250\n",
      "Train acc: 0.841625, Test_acc: 0.832500\n",
      "Train acc: 0.861292, Test_acc: 0.838750\n",
      "Train acc: 0.870958, Test_acc: 0.840000\n",
      "Train acc: 0.883042, Test_acc: 0.852500\n",
      "Train acc: 0.890250, Test_acc: 0.856250\n",
      "Train acc: 0.897250, Test_acc: 0.858750\n",
      "Train acc: 0.900292, Test_acc: 0.853750\n",
      "Train acc: 0.897250, Test_acc: 0.852500\n",
      "Train acc: 0.907042, Test_acc: 0.860000\n",
      "Train acc: 0.911208, Test_acc: 0.857500\n",
      "Train acc: 0.914125, Test_acc: 0.868750\n",
      "Train acc: 0.923875, Test_acc: 0.865000\n",
      "Train acc: 0.927458, Test_acc: 0.867500\n",
      "Train acc: 0.929292, Test_acc: 0.866250\n",
      "Train acc: 0.930500, Test_acc: 0.867500\n",
      "Train acc: 0.929875, Test_acc: 0.856250\n",
      "Train acc: 0.928458, Test_acc: 0.856250\n",
      "Train acc: 0.920000, Test_acc: 0.871250\n",
      "Train acc: 0.922125, Test_acc: 0.855000\n",
      "Train acc: 0.931958, Test_acc: 0.857500\n",
      "Train acc: 0.936083, Test_acc: 0.856250\n",
      "Train acc: 0.937917, Test_acc: 0.855000\n",
      "Train acc: 0.935083, Test_acc: 0.856250\n",
      "Train acc: 0.931375, Test_acc: 0.865000\n",
      "Train acc: 0.934958, Test_acc: 0.857500\n",
      "Train acc: 0.935000, Test_acc: 0.855000\n",
      "Train acc: 0.937875, Test_acc: 0.855000\n",
      "Train acc: 0.939458, Test_acc: 0.842500\n",
      "Train acc: 0.935833, Test_acc: 0.842500\n",
      "Train acc: 0.937167, Test_acc: 0.847500\n",
      "Train acc: 0.934042, Test_acc: 0.850000\n",
      "Train acc: 0.934708, Test_acc: 0.843750\n",
      "Train acc: 0.938375, Test_acc: 0.837500\n",
      "Train acc: 0.940333, Test_acc: 0.838750\n",
      "Train acc: 0.939458, Test_acc: 0.837500\n",
      "Train acc: 0.940583, Test_acc: 0.847500\n",
      "Train acc: 0.935417, Test_acc: 0.845000\n",
      "Train acc: 0.937542, Test_acc: 0.845000\n",
      "Train acc: 0.936000, Test_acc: 0.842500\n",
      "Train acc: 0.941042, Test_acc: 0.846250\n",
      "Train acc: 0.942000, Test_acc: 0.856250\n",
      "Train acc: 0.942958, Test_acc: 0.855000\n",
      "Train acc: 0.939875, Test_acc: 0.857500\n",
      "Train acc: 0.940917, Test_acc: 0.851250\n",
      "Train acc: 0.940708, Test_acc: 0.847500\n",
      "Train acc: 0.939500, Test_acc: 0.857500\n",
      "Train acc: 0.941792, Test_acc: 0.857500\n",
      "Time elapsed - 20.831631660461426 seconds.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "\n",
    "    h = h5py.File('data1.hdf5', 'r')\n",
    "    n1 = h.get('data_train') \n",
    "    n2 = h.get('data_test')\n",
    "\n",
    "    # Training cycle\n",
    "    total_batch_train = int(n1.shape[0] / batch_size)\n",
    "    total_batch_test = int(n2.shape[0] / batch_size)\n",
    "\n",
    "    for iter_num in range(num_epochs):\n",
    "        # variables for train and test accuracies\n",
    "        avg_acc_train = 0.\n",
    "        avg_acc_test = 0.\n",
    "        for i in range(total_batch_train):\n",
    "            train_x = n1[(i) * batch_size: (i + 1) * batch_size, ...]\n",
    "            train_y = labels_train[(i) * batch_size: (i + 1) * batch_size, :]\n",
    "\n",
    "            _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})\n",
    "            _label_train = [np.argmax(i) for i in _logits_train]\n",
    "            _label_train_y = [np.argmax(i) for i in train_y]\n",
    "            _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
    "            avg_acc_train += _accuracy_train\n",
    "\n",
    "\n",
    "        for j in range(total_batch_test):\n",
    "            test_x = n2[(j) * batch_size: (j + 1) * batch_size, ...]\n",
    "            test_y = labels_test[(j) * batch_size: (j + 1) * batch_size, :]\n",
    "\n",
    "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
    "            _label_test = [np.argmax(i) for i in _logits_test]\n",
    "            _label_test_y = [np.argmax(i) for i in test_y]\n",
    "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
    "            avg_acc_test += _accuracy_test\n",
    "\n",
    "        # print the train and test accuracies   \n",
    "        print(\"Train acc: %f, Test_acc: %f\" % (avg_acc_train/total_batch_train, avg_acc_test/total_batch_test))\n",
    "    duration = time.time() - start_time\n",
    "    print('Time elapsed - {} seconds.'.format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
